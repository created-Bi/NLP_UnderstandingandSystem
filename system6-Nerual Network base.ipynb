{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 神经网络基础"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 人工神经网络：\n",
    "     是指一系列受生物学和神经科学启发的数学模型。这些模型主要是通过对人脑的神经元网络进行抽象，构建人工神经元，并按照一定拓扑结构来构建人工神经元之间的连接，来模拟生物神经网络。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 神经元\n",
    "    神经元，是构成神经网络的基本单元，其主要是模拟生物神经元的结构和特性，接收一组输入信号并产生输出。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 神经元模型：\n",
    "   假设一个神经元接收$D$个输入 $x_1, x_2, ..., x_D$, 令向量 $X$（等于前面那个）, 来表示这组输入，并用净输入$ z∈R $表示一个神经元所获得的输入信号x的加权和。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ z=\\sum^D_{d=1}{w_dx_d + b} $$\n",
    "$$ = w^Tx+b $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ 其中w=[w_1; w_2; ... w_D ] 是D维的权重向量$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 激活函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ a = f(z) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 思考题"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 面试题：给定神经网络结构，计算总参数量"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input X：（$ M×N $） ，Output Y：（$ M，$） "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 线性层1（$ W_{N×N_1} $） ——> 激活层（？） ——> 线性层2（$ W_{N_1×N} $） ——> 损失（?） "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameters: $ M * N_1 $ + $ N_1 * N $    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 面试题：线性神经网络为什么会出现梯度消失现象？\n",
    "### Hint：结合反向传播的链式法则来思考"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans:\n",
    "\n",
    "    一些激活函数的导函数值在[0,1]间，所以如果连续使用此类激活函数的话，反向传播的时候会乘很多小于1的数值项，因此导致梯度无法很好传播"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 编程题：基于Numpy手工搭建神经网络 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  1. 神经网络模块"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 BaseNode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseNode(object):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(BaseNode, self).__init__()\n",
    "        self.params = {}\n",
    "        self.grads = {}\n",
    "        self.results = {}\n",
    "        self.name = None\n",
    "        self.inputs = []\n",
    "        self.outputs = []\n",
    "        self.is_trainable = True\n",
    "        for input_ in self.inputs:\n",
    "            input_.outputs.append(self)\n",
    "        \n",
    "    def forward(self):\n",
    "        raise NotImplementedError\n",
    "    def backward(self):\n",
    "        raise NotImplementedError\n",
    "    def __repr__(self):\n",
    "        return self.name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 LinearNode "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(BaseNode):\n",
    "    def __init__(self, input_dim, output_dim, name, bias=True):\n",
    "        super(Linear, self).__init__()\n",
    "        self.name = name\n",
    "        self.bias = bias\n",
    "        self.W = np.random.randn(input_dim, output_dim) / np.sqrt(input_dim)\n",
    "        self.params['w'] = self.W\n",
    "        if bias:\n",
    "            self.b = np.zeros([1, output_dim])\n",
    "            self.params['b'] = self.b\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # check type\n",
    "        assert isinstance(x, np.ndarray)\n",
    "        # x: batch_size, input_dim\n",
    "        batch_size = x.shape[0]\n",
    "        #print(\"linear batch_size\", batch_size)\n",
    "        y = np.dot(x, self.params['w'])\n",
    "        #print(\"linear y_hat: \", y.shape)\n",
    "        if self.bias and batch_size > 1:\n",
    "            y += np.tile(self.params['b'], (batch_size ,1))\n",
    "        self.results['x'] = x\n",
    "        self.results['y'] = y\n",
    "        return y\n",
    "    \n",
    "    def backward(self, pre_grad):\n",
    "        # check type\n",
    "        assert isinstance(pre_grad, np.ndarray)\n",
    "        # gradient of this layer\n",
    "        x = self.results['x']\n",
    "        batch_size, input_dim, output_dim = x.shape[0], x.shape[1], pre_grad.shape[1]\n",
    "        # gradient of w and b in current layer \n",
    "        #print(\"x: {}, pre_grad: {}\".format(x.shape, pre_grad.shape))\n",
    "        self.grads['w'] = np.dot(x.T, pre_grad) # input_dim, output_dim\n",
    "        if self.bias: self.grads['b'] = np.sum(pre_grad, axis=0, keepdims=False)# output_dim\n",
    "        # grad: batch_size, output_dim\n",
    "        grad_to_input = np.dot(pre_grad, self.W.T)\n",
    "        return grad_to_input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 ActivatedNode "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3.1 Sigmoid "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sigmoid(BaseNode):\n",
    "    def __init__(self):\n",
    "        super(Sigmoid, self).__init__()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x: batch_size, output_dim\n",
    "        sigmoid_value = 1 / (1 + np.exp(-1 * x))\n",
    "        self.results['sigmoid'] = sigmoid_value\n",
    "        return sigmoid_value\n",
    "    \n",
    "    def backward(self, pre_grad):\n",
    "        sigmoid_value = self.results['sigmoid']\n",
    "        grad_input = pre_grad * sigmoid_value * (1 - sigmoid_value)\n",
    "        return grad_input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3.2 Tanh "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tanh(BaseNode):\n",
    "    def __init__(self):\n",
    "        super(Tanh, self).__init__()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x: batch_size, output_dim\n",
    "        evalue = np.exp(x)\n",
    "        e_neg_value = np.exp(-1 * x)\n",
    "        tanh_value = (evalue - e_neg_value) / (evalue + e_neg_value)\n",
    "        self.results['tanh'] = tanh_value\n",
    "        return tanh_value\n",
    "    \n",
    "    def backward(self, pre_grad):\n",
    "        tanh_value = self.results['Tanh']\n",
    "        grad_input = pre_grad * (1 - tanh_value ** 2)\n",
    "        return grad_input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3.3 ReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLU(BaseNode):\n",
    "    def __init__(self):\n",
    "        super(ReLU, self).__init__()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x: batch_size, output_dim\n",
    "        mask = (x > 0).astype(float)\n",
    "        relu_value = mask * x\n",
    "        self.results['relu'] = mask\n",
    "        return relu_value\n",
    "    \n",
    "    def backward(self, pre_grad):\n",
    "        grad_input = pre_grad * self.results['relu']\n",
    "        return grad_input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3.4 Softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Softmax(BaseNode):\n",
    "    def __init__(self):\n",
    "        super(Softmax, self).__init__()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x: batch_size, output_dim\n",
    "        sum_of_x = np.sum(np.exp(x), axis=1) # batch_size\n",
    "        softmax_value = x / sum_of_x\n",
    "        self.results['softmax'] = softmax_value\n",
    "        return softmax_value\n",
    "    \n",
    "    def backward(self, pre_grad):\n",
    "        softmax_value = self.results['softmax']\n",
    "        grad_input = grad * softmax_value\n",
    "        return grad_input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 LossNode "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.4.1 L2Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class L2Loss(BaseNode):\n",
    "    def __init__(self):\n",
    "        super(L2Loss, self).__init__()\n",
    "    \n",
    "    def forward(self, x, ground_truth):\n",
    "        # x: batch_size, dim\n",
    "        diff = x - ground_truth\n",
    "        self.results['diff'] = diff\n",
    "        return np.sum(diff ** 2)\n",
    "    \n",
    "    def backward(self):\n",
    "        grad_input = 2 * self.results['diff']\n",
    "        return grad_input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.4.2 Binary Cross Entropy Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BinaryCrossEntropyLoss(BaseNode):\n",
    "    def __init__(self):\n",
    "        super(BinaryCrossEntropyLoss, self).__init__()\n",
    "    \n",
    "    def forward(self, x, label):\n",
    "        # x: batch_size, dim\n",
    "        batch_size = x.shape[0]\n",
    "        blabel = label.reshape([batch_size, 1])\n",
    "        bce = -(blabel * np.log(x) + (1 - blabel) * log(1 - x))\n",
    "        self.results['blabel'] = blabel\n",
    "        self.results['x'] = x\n",
    "        return np.sum(bce)\n",
    "    \n",
    "    def backward(self):\n",
    "        grad_input = self.results['blabel']\n",
    "        x = self.results['x']\n",
    "        grad_input = -(blabel / x - (1 - blabel) / (1 - x))\n",
    "        return grad_input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.4.3 Cross Entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossEntropy(BaseNode):\n",
    "    def __init__(self):\n",
    "        super(CrossEntropy, self).__init__()\n",
    "    \n",
    "    def forward(self, x, ground_truth):\n",
    "        # x: batch_size, dim\n",
    "        mask = (x * ground_truth) > 0\n",
    "        res = x * mask\n",
    "        self.results['x'] = x\n",
    "        self.results['crossentropy'] = mask\n",
    "        return np.sum(res, 1)\n",
    "    \n",
    "    def backward(self):\n",
    "        batch_size = self.results['x'].shape[0]\n",
    "        grad_input = np.zeros([batch_size, 1])\n",
    "        for i, x in enumerate(self.results['crossentropy']):\n",
    "            if x: grad_input[i] = self.results['x'][i] - 1\n",
    "            else: grad_input[i] = self.results['x'][i]\n",
    "        return grad_input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4.4 数据导入"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_boston"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = load_boston()['data']\n",
    "y_ = load_boston()['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(506, 13)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BostonData(BaseNode):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(BostonData, self).__init__()\n",
    "        self.linear1 = Linear(input_dim, hidden_dim, name='linear1')\n",
    "        self.relu = ReLU()\n",
    "        self.linear2 = Linear(hidden_dim, output_dim, 'linear2')\n",
    "        #self.sigmoid = Sigmoid()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.linear1.forward(x)\n",
    "        #print('1', x.shape)\n",
    "        x = self.relu.forward(x)\n",
    "        #print('2', x.shape)\n",
    "        x = self.linear2.forward(x)\n",
    "        #print('3', x.shape)\n",
    "        return x\n",
    "        \n",
    "    def backward(self, loss):\n",
    "        grad = self.linear2.backward(loss)\n",
    "        grad = self.relu.backward(grad)\n",
    "        grad = self.linear1.backward(grad)\n",
    "    \n",
    "    def update(self, learning_rate):\n",
    "#         print(type(self.linear2.grads))\n",
    "        self.linear2.params['w'] += (-1) * learning_rate * self.linear2.grads['w']\n",
    "        if self.linear2.bias: self.linear2.params['b'] += (-1) * learning_rate\n",
    "        self.linear1.params['w'] += (-1) * learning_rate * self.linear1.grads['w']\n",
    "        if self.linear2.bias: self.linear1.params['b'] += (-1) * learning_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = 13\n",
    "hidden_dim = 20\n",
    "output_dim = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm_notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:10: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "333f81a9bec04010bde4ef691e3c0352",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=4000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 0th epoch, loss:2628.9749039869967\n",
      "The 1th epoch, loss:113.64704979615338\n",
      "The 2th epoch, loss:108.0415478725296\n",
      "The 5th epoch, loss:105.3842555900598\n",
      "The 6th epoch, loss:104.91968928064212\n",
      "The 8th epoch, loss:82.09414645615095\n",
      "The 10th epoch, loss:72.73063796861447\n",
      "The 12th epoch, loss:55.77339101794017\n",
      "The 67th epoch, loss:51.13098250487756\n",
      "The 138th epoch, loss:49.06906214268378\n",
      "The 175th epoch, loss:44.65652351201641\n",
      "The 289th epoch, loss:41.37621045756401\n",
      "The 667th epoch, loss:34.16499700366162\n",
      "The 1927th epoch, loss:29.56364249272107\n",
      "The 3717th epoch, loss:28.295813330297587\n",
      "The 3776th epoch, loss:28.0474365451324\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = BostonData(input_dim, hidden_dim, output_dim)\n",
    "criterion = L2Loss()\n",
    "\n",
    "learning_rate = 0.00001\n",
    "epoch = 4000\n",
    "batch_size = 128\n",
    "losses = []\n",
    "min_loss = np.float('inf')\n",
    "\n",
    "for e in tqdm_notebook(range(epoch)):\n",
    "    loss = 0\n",
    "    \n",
    "    for b in range(batch_size):\n",
    "        index = np.random.choice(range(len(X)))\n",
    "        input_vetor = X[index].reshape(1, input_dim)\n",
    "#         print(input_vetor.shape) \n",
    "        y_hat = model.forward(input_vetor)\n",
    "        output_vector = y_[index].reshape(1, output_dim)\n",
    "#         print(\"y_hat shape: {}, y_ shape: {}\".format(y_hat.shape, output_vector.shape))\n",
    "        cost = criterion.forward(y_hat, output_vector)\n",
    "        cost_grad = criterion.backward()\n",
    "        model.backward(cost_grad)\n",
    "        model.update(learning_rate)\n",
    "        \n",
    "        loss += cost\n",
    "    ave_loss = loss / batch_size\n",
    "    if min_loss > ave_loss:\n",
    "        min_loss = ave_loss\n",
    "        print(\"The {}th epoch, loss:{}\".format(e, ave_loss))\n",
    "    losses.append(ave_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f82ccb66ad0>]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXgAAAD2CAYAAADcUJy6AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAZE0lEQVR4nO3de3hV9Z3v8fc3N8I1kAtXgQgUUBEcGvCGCliotsgZZWqt8tSh43g8do7niLXT01Orzjwe9alHjx47Tu20M30ctJ3qeFSwIjdbcSoYqgFF7iCEa0Ju5J7s/Tt/7G1k7ayYnUB2spaf1/Pkce/vWnut716GT1bWb/+yzDmHiIiET1pvNyAiIj1DAS8iElIKeBGRkFLAi4iElAJeRCSkMnq7gU/l5+e7wsLC3m5DRCRQtmzZUu6cK/Bb1mcCvrCwkOLi4t5uQ0QkUMzsk46W6RKNiEhIKeBFREJKAS8iElIKeBGRkFLAi4iElAJeRCSkFPAiIiEV+IDfdfwUj7+5k/Lapt5uRUSkTwl8wO8+XstT6/dQUdfc262IiPQpgQ94ERHxp4AXEQkpBbyISEiFJuB1a1kREa/AB7xZb3cgItI3BT7gRUTEX1IBb2a/MrN3zexVM5tlZqVmtjH+NcXMss1spZmVmNlzFtOu1tNvRkREPtNpwJvZHCDDOXcJMAQYBTzjnJsT/9oJLAVKnXMzgGHAgg5qIiKSIsmcwR8Hnjxt/WHAEjPbbGYvxc/M5wNr4uusB+Z1UOsxDo2yioicrtOAd87tds5tNrPrgSiwA7jPOTeb2Nn8VUAeUB1/SQ2Q20HNw8xuN7NiMysuKyvr1hvQdR8REX/JXoNfDNwFXAfsAdbGFx0AhgPlQE68lhN/7lfzcM4965wrcs4VFRT43jNWRES6KZlr8COBe4FFzrlTwHLgJjNLA6YBHwLrgIXxl8wHNnRQExGRFEnmDP5WYpdiVpvZRqAeWAZsAl52zm0HVgBjzGwrUEEs3P1qIiKSIhmdreCcexR4NKH8UMI6TcCihHX8aj1GM1lFRLwCP9FJn64XEfEX+IAXERF/CngRkZBSwIuIhFRoAl6DrCIiXiEIeI2yioj4CUHAi4iIHwW8iEhIKeBFREIqNAGvPxcsIuIV+IDXTFYREX+BD3gREfGngBcRCSkFvIhISIUm4DWTVUTEK/ABrzFWERF/gQ94ERHxp4AXEQkpBbyISEgp4EVEQirwAW+ayioi4ivwAS8iIv4U8CIiIRWagNdEJxERr8AHvK7Ai4j4C3zAi4iIPwW8iEhIKeBFREIqNAGvW/aJiHgFPuA1z0lExF9SAW9mvzKzd83sVTMbZGYrzazEzJ6zmOxkaj39ZkRE5DOdBryZzQEynHOXAEOA7wClzrkZwDBgAbA0yZqIiKRIMmfwx4EnT1v/AWBN/Pl6YB4wP8maiIikSKcB75zb7ZzbbGbXA1HgfaA6vrgGyAXykqx5mNntZlZsZsVlZWVn9EY0k1VExCvZa/CLgbuA64BjQE58UQ5QHv9KpubhnHvWOVfknCsqKCjo1hvQlX0REX/JXIMfCdwLLHLOnQLWAQvji+cDG7pQExGRFEnmDP5WYBSw2sw2ApnAGDPbClQQC/IVSdZERCRFMjpbwTn3KPBoQvlnCc+bgEVJ1EREJEUCP9HpUxpjFRHxCnzAm/5gsIiIr8AHvIiI+FPAi4iElAJeRCSkQhPwTlNZRUQ8gh/wGmMVEfEV/IAXERFfCngRkZBSwIuIhFRoAl5DrCIiXoEPeI2xioj4C3zAi4iIPwW8iEhIKeBFREIqNAGviawiIl6BD3jTTVlFRHwFPuBFRMSfAl5EJKQU8CIiIRWigNcoq4jI6QIf8BpiFRHxF/iAFxERfwp4EZGQUsCLiIRUaAJeM1lFRLwCH/CayCoi4i/wAS8iIv4U8CIiIaWAFxEJqaQC3swyzey1+ONZZlZqZhvjX1PMLNvMVppZiZk9ZzHtaj35RjTGKiLi1WnAm1l/YAuwIF4aBjzjnJsT/9oJLAVKnXMz4ssXdFA760xzWUVEfHUa8M65BufcdKA0XhoGLDGzzWb2UvzMfD6wJr58PTCvg5qIiKRId67B7wHuc87NBkYBVwF5QHV8eQ2Q20HNw8xuN7NiMysuKyvrRisiItKR7gT8AWDtaY+HA+VATryWE3/uV/Nwzj3rnCtyzhUVFBR0oxUREelIdwJ+OXCTmaUB04APgXXAwvjy+cCGDmo9RjNZRUS8uhPwTwPLgE3Ay8657cAKYIyZbQUqiIW7X+2s00xWERF/Gcmu6JybFP/vUWBuwrImYFHCS/xqIiKSIproJCISUgp4EZGQCk3AO42yioh4BD7gNcYqIuIv8AEvIiL+FPAiIiGlgBcRCanQBLyGWEVEvIIf8BplFRHxFfyAFxERXwp4EZGQUsCLiIRUaAJeE1lFRLwCH/C6J6uIiL/AB7yIiPhTwIuIhFRoAt5pqpOIiEfgA1637BMR8Rf4gBcREX8KeBGRkFLAi4iEVHgCXmOsIiIegQ94jbGKiPgLfMCLiIg/BbyISEgp4EVEQio0Aa8xVhERr8AHvGkqq4iIr8AHvIiI+FPAi4iEVFIBb2aZZvZa/HG2ma00sxIze85ikqr17FsREZHTdRrwZtYf2AIsiJeWAqXOuRnAsHg92VqP0S37RES8Og1451yDc246UBovzQfWxB+vB+Z1oXbW6fcCERF/3bkGnwdUxx/XALldqHmY2e1mVmxmxWVlZd1oRUREOtKdgC8HcuKPc+LPk615OOeedc4VOeeKCgoKutGKiIh0pDsBvw5YGH88H9jQhZqIiKRIdwJ+BTDGzLYCFcSCPNlaj9E9WUVEvDKSXdE5Nyn+3yZgUcLiZGtnncZYRUT8aaKTiEhIKeBFREJKAS8iElKhCXjNZBUR8Qp8wGsmq4iIv8AHvIiI+FPAi4iElAJeRCSkQhPwGmMVEfEKQcBrlFVExE8IAl5ERPwo4EVEQkoBLyISUqEJeKeprCIiHoEPeM1kFRHxF/iAFxERfwp4EZGQUsCLiIRUaAJeQ6wiIl6BD3iNsYqI+At8wIuIiD8FvIhISCngRURCKjwBr1FWERGPwAe8aSqriIivwAe8iIj4U8CLiISUAl5EJKRCE/BOo6wiIh6BD3gNsYqI+OtWwJvZLDMrNbON8a8ZZrbSzErM7DmLyU6sne3mRUSkY909gx8GPOOcm+OcmwPMAkqdczPiyxYAS31qIiKSIhndfN0wYImZ/SfgENAMvBhfth6YB4wHXkqovdn9VkVEpCu6ewa/B7jPOTcbGAXcAFTHl9UAuUCeT83DzG43s2IzKy4rK+tmKzG6JauIiFd3A/4AsPa0x1EgJ/48ByiPfyXWPJxzzzrnipxzRQUFBd1qRFf2RUT8dTfglwM3mVkaMA24B1gYXzYf2ACs86mJiEiKdDfgnwaWAZuAl4FfAGPMbCtQQSzcV/jUREQkRbo1yOqcOwrMTSgvSnje5FMTEZEUCfxEp09pkFVExCvwAW+ayyoi4ivwAS8iIv4U8CIiIaWAFxEJqdAEvMZYRUS8Ah/wmskqIuIv8AEvIiL+FPAiIiGlgBcRCanQBLzTVFYREY/QBLyIiHgp4EVEQkoBLyISUqEJeF2BFxHxCnzAa6KTiIi/wAe8iIj4U8CLiISUAl5EJKRCE/AdzXNqbIkQiWoIVkS+eAIf8J3dsm/qfW9w1wvvc6y6MUUdiYj0DYEP+ET1za0cPFnvqa3adpRLHl5HRV1zL3UlIpJ6Gb3dwNlyx79uAeCcYf0prWzgwCNf5z/2lnvWqWloIXdgVm+0JyKScoE/g6+s956Vl1Y2APAv7+zn5p9v8ix7fM0u3jtQQeEPVvHHvSc5Wt3APf9WQmNLxLOec46j1Q2UnWrq2eZFRHpQ4M/gTzW2+NYfeG17u9qrJUd4teQIAN/6+btt9cHZGew5Ucud8yby/KaDvLOnnMr62HYPPPJ1WiJRPjhUxeQRgxmYlU5dc4SSQ1W88dEx/stVEzlW08jIIdn8ZPVOhvTP4O8WT2NfeR0jhvRjb1kde0/U8pXzRrD9aA2XTsxr2++LW0r53m9LePPuKwGYPGJwu54bWyJkZ6YDsLeslr0nall4wchuHq2Y1kgUB2Smx36+f3Coikg0ypfH557RdkVS5fR/F9Ix6yt/ZreoqMgVFxd3+XXRqGPCD1/vgY5ill1eSEVdM698cOSsbG/t8iu5+zclPH7jDB547SPe2XPSs3zVXXMYmJXB73eV8Q9v7eF4TRO5A7N4/a4ruOThdQA8f9vFvHegkjUfHyMShY+P1vBf509iX1kdSy8Zz4Xn5JCRZnzvtyV8/6tTGZc3gNqmVjbvP8mXx+cy48E3Adj2wEIe/t0Ont900NPD29+fR0a6cenD67lh5hgeXHwB/1Zcyt+v3M4tF4/joesvbFu3oq6ZFe9+wnfnTSItzdh1/BS/23aM//aVL1HX1IoDBvXznke0RKKkx6cgp6V9NkgejTp++c5+vjV7HAMTXlPd0EJO/8y2502tEZzD9x/5Y6t3EnWO718z1ff/waZ9J2mJOIoKh3G8ppHxeQN914PYb3M1ja3UNLRQMLhfh6HSGolSUd/M8MHZ7CurZd3HJ/jrKye0Wy8adWw/WsPwwf14ct1u7r/uArIy0nhxSymjcrK5fFK+7/YjUceDr33EvCnDmTulgPLaZgoG9/Ndd395HaOHZtMvI9Zrc2uUuqZWfvnOfm6+eByjcvq3e01Dc4Tm1igOx9ABWTS2RNjySWWH/SQ6UtXA4OwMBmdndr5yEiJRR5qB+UxV37i7nKW/2MSLd1xKUWHspOT1bUc51djCN2eNOyv7/zyfnKxj6IAsz/djbzKzLc65It9lQQ94gMIfrDrL3QRf3sAsTsYHlScWDGRvWd1Z2/YFo4fw0ZEaT21IdgYP3zCd7z7/py5ta+3yK/n15kP808b9nm2NyunPqcYWjiR8+mnt8ispPlDJD/59m+/2/nHpTO741896GJCVzut3XcHhqgZ+sXE/xQcqqGls9bzmK+eNIG9gFr8pPgTA1gcWcriygcdW72TdjhOedf/2mqn86WAl7x+s5JaLxzP9nBxKSqt5at1uADb/8Gpm/691besPzs7gtjkT+NqFI3nlgyM8vWFPrN4vg1NNrcyfOpwnvnlR2w/dHX9/DVPvewOA9fdcxSO/28Hoof0ZnzeABxN+K33xjkvJHZjFWzvLWHZ5IU+s2cVT62PbH5vbn2/NHsdlE/P532/u5O3dsfGoYQMy+eascVTVN/Pr9w5x/3XnM3RAJnf/pqRtu098cwab91fywuaDzJ1SwO1XTGDWubkY0NAS4cIHYr0uu7yQq6eOYM6X8tv+Db72N3N4a+cJbpw1ltyBWRypamDX8Vp+umEPk0cM4uvTR7Nq6xHmTx3ONdNGsfv4KaoaWpg8fDBzH9vAjbPGsmTmOSx84g/MmZTPwzdcSP6gfryw+SA/Wb2TG4vOYeXWo5ysa+ber07hu/Mm0dgSaTtmD10/jUjUUXKomqumFFBR28StlxW2/aCIRh3NkWiHP6jf3XeSDTtPsHzBZA6erCd/UD/6Z6VzoqaJjHRja2lV2/fXx393Df2zYtv5P2t3cf6oIW2/XR+rbuSFzQd5besR/uUvZ7P4pxu5Z+EUbvizMVxw/2qunTaSq88bQTTquGHmGDLSu3+1XAEvIl94S2aew0t/Km17/r2Fk3nszV1ntM1Hl1zIqm3H+MOuMgCmn5PD1tLqLm+n5McLyRnQvd8IPi/gAz/ICjBj7NC2x1ln8JNQRMLr9HAHzjjcAf72pW1t4Q50K9wBnnv3wBn34icUafjLW4t4dMmFHHjk6+x66Fr+/c7LyMpI48eLzmf3Q9dy1/xJlNy/sG39n3/7sx92k0cM6nT7z9wyk7++4ty254tnjO70NT/5i+ksu7ywXX1CfsfXexNNGt55b70hPU1/wlPkbEq8bHi29NglGjPLBl4ExgJbgW+7z9nZmVyi6Y53953kvFFDyOmfSXNrlMx0w8w4XNXAgMx0jp9q5B/f2ssjS6a3Xa9rbo0SdY7szHRO1sYGP0srGxibO6Btu/XNrfTPTPcMDn10pJoJ+YPartfVNbWSnmZkpqdhfDbQ+Jf/vJm3dpax9YGFDIkPVjU0R9hfXsf4vAHUNbey61gtL7x3kCduvIhth6sYmzuA9w9Wsb+8jpsvHsevNx/k25cWkp5m1Da2svVwNU0tEa4+bwQPv/4xt10xgZE52Z5j0dgSoaE5wrCBWUSijj/uPcnlk/JYte0oC84fQV1ThMfX7OT+6y4gI83a3ltTa4S6pgiZ6cZv3jvE3CnDOTd/ID/6fx/ywuaDPHT9NBaeP5L8QVkUf1KJc3Cwop5tpVVMGj6I+175iKe+9Wc45yivbWbxjNH809v7+Nkf9nHrpeP5j70nuXvBZD4+WsP/Xb+HLT/6CjWNrTy5dhfXTBvFik2f8MOvnce1T77NM7fM5KMjNbyzt5z3D1ZxyYRc/vyiMVTUN3Pn3EkcKK/jwyPVvL2rnB3HaigpreaKL8UGEL9RNJbDlQ08+saOtmPyD7fMZOiATG7++SYum5jHRWOHkpFmNEWi/Oz3+3j5zss4HL++PHPcUNLTjJFDsnlh8yEGZKWTPyiLwvyBfP/FrVQ1tPDYN2bw4pZS9p6o5cHFFxB1jldLjnDDzDH81a+K2/7UxvIFk5lVmEvUOarqW8hIN2YX5jIoO4Mfv/IRd86dyNjcARwor6O2qZWDFfXcuaLjcY//fOUE3t5dzvaj3jGTDd+by+vbjvKT1Tvbak/edBHXTR/d7kMLz/3VbJ5ev4dN+yvon5lOQ/xjxaNzstuNkQDc+9Upnu0CDB/cj0f/YjrL/vm9dutPKBhIVnoaO46darfszbuvZOETf2h7/tD10/ifL3/Y4ftNNHRAJg3NEZpaox2uM2n4IPacqE16m35um3OuZxypI1NGDGbn8fbv87Y55/KjRed3a9+9cg3ezG4Dipxzd5jZSuAp59ybHa2f6oDvi5pbozS0RPrM6PwXzYeHq6msb+aKLxW01UoOVTF11OC2T6QEQWskSn1LpO0koTPOOc8Jye7jpxjQL4MxQ9t/2gagtqmVSMS1u2ZcWddMcyTKiCHZvq/z63PLJ5VcPCEP5xxltU0Myc5sO4Gqb454Tp4StUSiZKQZR6sbOVLVwIgh2W3rf97HKPeW1TJmaP92y0/UNJKZnsawDiZDllbW88oHR7hz7kTMjKbWCM/+fh+XTsxr+zTP0eqGtk8pNbdG2X3iFBeMzvnc4/DKB4eZMnIwU0cO+dz1OtJbAf888JJz7iUzWw4UOOf+R0frK+BFRLqutwZZ84BPRxxqgHazaMzsdjMrNrPisrKyxMUiInIGejLgy4FPfzfJiT/3cM4965wrcs4VFRQUJC4WEZEz0JMBvw749KMr84ENPbgvERFJ0JMBvwIYY2ZbgQpigS8iIinSY39szDnXBCzqqe2LiMjnC8VEJxERaU8BLyISUgp4EZGQ6jN/TdLMyoBPuvnyfHw+htkH9NW+oO/2pr66Rn11TRj7Gu+c8/2ceZ8J+DNhZsUdzeTqTX21L+i7vamvrlFfXfNF60uXaEREQkoBLyISUmEJ+Gd7u4EO9NW+oO/2pr66Rn11zReqr1BcgxcRkfbCcgYvIiIJFPAiIiEV6IA3s2wzW2lmJWb2nJ1+W5rU9TDLzErNbGP8a0ZiT6nu08wyzey1+ON2+062loLeEo/dlN7ozcx+ZWbvmtmrZjaorxyvhL76yrHKMLPfmtk7ZvbLvvL95dNXnzhep/W33MzWmlm+mb1tZtvM7JH4sqRq3RHogAeWAqXOuRnAMGBBL/QwDHjGOTfHOTcHmOXTU8r6NLP+wJbT9uG372RrPd2b59g553amujczmwNkOOcuAYYA30ly/z16vHz6GkUvH6u4PwdKnHOXx3v6myR7SHVf8+gbxwszGw/cGn/634FVwAzgWjOb3IValwU94OcDa+KP1xP7n5pqw4AlZrbZzF4CrvbpKWV9OucanHPTgdJ4yW/fydZ6ujfPsYufPaW6t+PAk/HHacADSe6/p49XYl994VgBvAE8bmYZwFBgZpI9pLovo28cL4j9f/z0dqXzgTXOuSjw+9N7SKLWZUEP+E5vC5gCe4D7nHOziZ053ODTU2/26bfvZGs9LfHYXZXq3pxzu51zm83seiAKvJ/k/nv0ePn0tYNePlbxvmqdc/XAO8R+CPWJ7y+fvtbSB46Xmd0MlADb46WUHq+gB3yntwVMgQPEvpk+fRylfU+92affvpOt9bQDeI/d8N7ozcwWA3cB1wHHktx/jx+vhL720DeOVZ6Z9QMuI/ZbxbQke0h1X4X0geNF7J4YVwO/Br5M7G/OpOx4BT3g+8JtAZcDN5lZGrFv9nt8eurNPv32nWytpyUeuw9T3ZuZjQTuBRY55051Yf89erx8+ur1YxV3D/AN51wEqAceSrKHVPf1I/rA8XLO3Rwfm7uJ2PjTT4GF8b6uOr2HJGpdFvSA7wu3BXwaWAZsAl4GfuHTU2/26bfvZGs9zXPsnHPbe6G3W4n9Cr/azDYCmUnuv6ePV2Jf9fT+sYJYQH3HzP4InCT57/dU97WIvnG8Ej0FfA3YCqxyzu3pQq3LNJNVRCSkgn4GLyIiHVDAi4iElAJeRCSkFPAiIiGlgBcRCSkFvIhISP1/Kzu68w8xQDUAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. dropout机制 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ans：\n",
    "\n",
    "    1. Dropout机制用于丢弃神经网络结点中的一部分，使其不发生作用；\n",
    "    2. 伪代码：\n",
    "        parameters is the parameters in the layer\n",
    "        random_index = random.random(parameters.size())\n",
    "        for ind in random_index:\n",
    "          y[ind] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. ELU层 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ELU(BaseNode):\n",
    "    def __init__(self):\n",
    "        super(ELU, self).__init__()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x: batch_size, output_dim\n",
    "        mask = (x > 0).astype(float)\n",
    "        mask_2 = (x <= 0).astype(float)\n",
    "        elu_value = mask * x + mask_2 * (np.exp(x) - 1)\n",
    "        self.results['elu'] = mask + mask_2 * np.exp(x)\n",
    "        return elu_value\n",
    "    \n",
    "    def backward(self, pre_grad):\n",
    "        grad_input = pre_grad * self.results['elu']\n",
    "        return grad_input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 思考题：如何实现自动求导 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ans：\n",
    "    1. 前向传播时，把每个node的输入结点的输出结点设置为自身，所以最后会形成一张计算图；\n",
    "    2. 在计算图上实现拓扑排序，就可以得到前向传播的方向，逆序便得到反向传播方向。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
